{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the IA\n",
    "\n",
    "### Import python libraries and our libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8d19677fc016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ia4business/Part 2 - Minimizing Costs/blog/brain.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "import environment\n",
    "import brain\n",
    "import dqn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility seeds setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(85)\n",
    "rn.seed(12345)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.3 #our system will take 30% exploration(action random selection) and 70% explotation\n",
    "number_actions = 5\n",
    "direction_boundary = (number_actions -1)/2 #intermediate point (our boundary)\n",
    "number_epochs = 100\n",
    "max_memory = 3000\n",
    "batch_size = 512\n",
    "temp_step = 1.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the environment with a class object Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = environment.Environment(optimal_temp = (15.0, 25.0), initial_month = 0, initial_number_users = 15, initial_rate_data = 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the brain with a class object Brain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = brain.Brain(learning_rate = 0.00001, number_actions = number_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building DQN model with a class object DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = dqn.DQN(max_memory = max_memory, discount_factor = 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.train = train\n",
    "model = brain.model\n",
    "early_stopping = True\n",
    "patience = 10\n",
    "best_total_reward = -np.inf\n",
    "patience_count = 0\n",
    "\n",
    "if (env.train):\n",
    "    # STARTING EPOCH BUCLE (1 Epoch = 5 Mouths)\n",
    "    for epoch in range(1, number_epochs):\n",
    "        # STARTING ENVIRONMEN VARIABLES AND TRAINING BUCLE\n",
    "        total_reward = 0\n",
    "        loss = 0.\n",
    "        new_month = np.random.randint(0, 12)\n",
    "        env.reset_env(new_month = new_month)\n",
    "        game_over = False\n",
    "        current_state, _, _ = env.give_env() #we only want current_state return from give_env method\n",
    "        timestep = 0\n",
    "        # INICIALIZATION TIMESTEPS BUCLE(Timestep = 1 minute) AT ONE EPOCH\n",
    "        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):\n",
    "            # RUNNING NEXT ACTION BY EXPLORATION\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, number_actions)\n",
    "                if (action - direction_boundary < 0):\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "                energy_ai = abs(action - direction_boundary) * temp_step\n",
    "            \n",
    "            # RUNNING NEXT ACTION BY EXPLOTATION\n",
    "            else:\n",
    "                q_values = model.predict(current_state)\n",
    "                action = np.argmax(q_values[0])\n",
    "                if (action - direction_boundary < 0):\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "                energy_ai = abs(action - direction_boundary) * temp_step\n",
    "               \n",
    "            # UPDATING ENVIRONMENT AND REACHING NEXT STATE\n",
    "            next_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep/(30*24*60)))\n",
    "            total_reward += reward\n",
    "\n",
    "            # SAVING NEW TRANSITION IN MEMORY\n",
    "            dqn.remember([current_state, action, reward, next_state], game_over)\n",
    "\n",
    "            # GETTING INPUTS AND TARGETS BLOCKS\n",
    "            inputs, targets = dqn.get_batch(model, batch_size)\n",
    "\n",
    "            # CALCULATING LOOST FUNCTION WITH THE WHOLE INPUT AND TARGET BLOCK\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            timestep += 1\n",
    "            current_state = next_state\n",
    "\n",
    "        # PRINTING RESULTS AT THE END OF EPOCH\n",
    "        print(\"\\n\")\n",
    "        print(\"Epoch: {:03d}/{:03d}.\".format(epoch, number_epochs))\n",
    "        print(\" - Total Energy spended by IA: {:.0f} J.\".format(env.total_energy_ai))\n",
    "        print(\" - Total Energy spended by no-IA: {:.0f} J.\".format(env.total_energy_noai))\n",
    "\n",
    "        # EARLY STOPPING\n",
    "        if early_stopping:\n",
    "            if (total_reward <= best_total_reward):\n",
    "                patience_count += 1\n",
    "            else:\n",
    "                best_total_reward = total_reward\n",
    "                patience_count = 0\n",
    "\n",
    "            if patience_count >= patience:\n",
    "                print(\"Early method execution.\")\n",
    "                break\n",
    "\n",
    "\n",
    "        # Saving model for the future\n",
    "        model.save(\"model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
