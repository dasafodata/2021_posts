{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows optimization in a warehouse with *Q-learning*\n",
    "\n",
    "Our client has asked us to implement an algorithm. This robot has to run through\n",
    "a warehouse in the most efficient way possible.\n",
    "This is de warehouse plan:\n",
    "\n",
    "<img src=\"Recursos/almacen.jpg\" width=\"500\">\n",
    "\n",
    "A Q-learning algorithm must be applied. This is a model-free reinforcement\n",
    "learning algorithm.\n",
    "\n",
    "Q-learning is a values-based learning algorithm. Value based algorithms updates\n",
    "the value function based on an equation(particularly Bellman equation). Whereas\n",
    "se sign column(bar)\n",
    "the other type, policy-based estimates the value function with a greedy policy\n",
    "obtained from the last policy improvement.\n",
    "\n",
    "Q-learning uses **Temporal Differences(TD)** to estimate the value of $Q(s,a)$.\n",
    "Temporal difference is an agent learning from an environment through episodes\n",
    "with no prior knowledge of the environment:\n",
    "\n",
    "$$TD_t(s_t,a_t) = R(s_t,a_t)+ \\gamma max_a(Q(s_{t+1},a)) - Q(s_t,a_t)$$\n",
    "\n",
    "The Q-function uses the **Bellman equation**:\n",
    "\n",
    "$$Q_t(s_t,a_t) = Q_{t-1}(s_t,a_t) + \\alpha TD_t (s_t,a_t)$$\n",
    "\n",
    "* You can get more information about **Q-learning** from here: **[A Beginners\n",
    "Guide to Q-Learning](https://towardsdatascience.com/a-beginners-guide-to-q-\n",
    "learning-c3e2a30a653c)**\n",
    "\n",
    "## **1 Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 Configuration of $\\gamma$ coefficent*(Discount Rate)* and $\\alpha$\n",
    "se sign column(bar)\n",
    "coefficent*(Learning Rate)***\n",
    "se sign column(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.75\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 Definition of the environment**\n",
    "\n",
    "### **3.1** Definition of the states\n",
    "\n",
    "The state is the location of our robot in the warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "location_to_state = {'A': 0,\n",
    "                     'B': 1,\n",
    "                     'C': 2,\n",
    "                     'D': 3,\n",
    "                     'E': 4,\n",
    "                     'F': 5,\n",
    "                     'G': 6, \n",
    "                     'H': 7, \n",
    "                     'I': 8,\n",
    "                     'J': 9,\n",
    "                     'K': 10,\n",
    "                     'L': 11}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of actions\n",
    "\n",
    "The action is the next position where our robot can move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "actions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of rewards\n",
    "\n",
    "We define the rewards, creating a reward matrix, where the rows correspond to\n",
    "the current states $s_t$, the columns correspond to the actions $a_t$ that lead\n",
    "to the next state $s_{t} + 1$, and the cells contain the rewards $R(s_t, a_t)$.\n",
    "If a cell $(s_t, a_t)$ has a $1$, that means that we can perform action a from\n",
    "the current state $s_t$ to get to the next state $s_t + 1$. If a cell $(s_t,\n",
    "a_t)$ has a $0$, that means we cannot perform the action at from the current\n",
    "state $s_t$ to get to any next state $s_t + 1$.\n",
    "\n",
    "**- Columns are *The action* and the rows are *The State*:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "R = np.array([[0,0,0,0,1,0,0,0,0,0,0,0], # A\n",
    "              [0,0,1,0,0,1,0,0,0,0,0,0], # B\n",
    "              [0,1,0,1,0,0,0,0,0,0,0,0], # C\n",
    "              [0,0,1,0,0,0,0,1,0,0,0,0], # D\n",
    "              [1,0,0,0,0,1,0,0,0,0,0,0], # E\n",
    "              [0,1,0,0,1,0,0,0,0,1,0,0], # F\n",
    "              [0,0,0,0,0,0,0,0,0,0,1,0], # G\n",
    "              [0,0,0,1,0,0,0,0,0,0,0,1], # H\n",
    "              [0,0,0,0,0,0,0,0,0,1,0,0], # I\n",
    "              [0,0,0,0,0,1,0,0,1,0,1,0], # J\n",
    "              [0,0,0,0,0,0,1,0,0,1,0,0], # K\n",
    "              [0,0,0,0,0,0,0,1,0,0,0,0]])# L "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4** Building a AI with Q-learning\n",
    "\n",
    "**- Inverse transformation from states to locations (1,2,3,...)--->(A,B,C,...):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "state_to_location = {state : location for location, state in location_to_state.items()}\n",
    "\n",
    "print(state_to_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Create a final function that returns us the optimal route:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "def route(starting_location, ending_location):\n",
    "    R_new = np.copy(R)\n",
    "    ending_state = location_to_state[ending_location]\n",
    "    R_new[ending_state, ending_state] = 1000 #to give 1000(points) to the final state \n",
    "    \n",
    "    Q = np.array(np.zeros([12, 12])) #Q-value\n",
    "    for i in range(1000):\n",
    "        current_state = np.random.randint(0, 12)#random number between 0-12\n",
    "        playable_actions = []\n",
    "        for j in range(12): #thorugh the columns\n",
    "            if R_new[current_state, j] > 0:\n",
    "                playable_actions.append(j)\n",
    "        next_state = np.random.choice(playable_actions) #current_action\n",
    "        TD = R_new[current_state, next_state] + gamma*Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state] #Temporal Diferences(TD)\n",
    "        Q[current_state, next_state] = Q[current_state, next_state] + alpha*TD #Bellman equation\n",
    "    \n",
    "    route = [starting_location]\n",
    "    next_location = starting_location\n",
    "    while(next_location != ending_location):\n",
    "        starting_state = location_to_state[starting_location]\n",
    "        next_state = np.argmax(Q[starting_state, ])\n",
    "        next_location = state_to_location[next_state]\n",
    "        route.append(next_location)\n",
    "        starting_location = next_location\n",
    "    return route\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5** Final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "def best_route(starting_location, intermediary_location, ending_location):\n",
    "    return route(starting_location, intermediary_location) + route(intermediary_location, ending_location)[1:]\n",
    "\n",
    "print(\"Chosen route:\")\n",
    "print(best_route('L', 'B', 'G'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
